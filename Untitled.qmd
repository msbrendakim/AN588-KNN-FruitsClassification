---
title: "KNN Analysis"
format: html
editor: visual
---

## Fruit Sorting Data Exploration

Required libraries

```{r}
library(readr)   
library(ggplot2)    
library(dplyr)    
library(class)  
library(caret)   
```

Read the data

```{r}
getwd()
fruit_data <- read.delim("fruit_data_with_colors.txt", sep="\t", header=TRUE)
```

Data Exploration

```{r}
head(fruit_data)
summary(fruit_data)
str(fruit_data)
```

Check class distribution

```{r}
table(fruit_data$fruit_name)

# Convert fruit_label and fruit_name to factors
fruit_data$fruit_label <- as.factor(fruit_data$fruit_label)
fruit_data$fruit_name <- as.factor(fruit_data$fruit_name)

# Basic visualization
ggplot(fruit_data, aes(x = width, y = height, color = fruit_name)) +
  geom_point(size = 3, alpha = 0.7) +
  labs(title = "Fruit Classification by Width and Height",
       x = "Width (cm)", y = "Height (cm)", color = "Fruit Type") +
  theme_minimal()
```

```{r}
# feature exploration
ggplot(fruit_data, aes(x = fruit_name, y = mass, fill = fruit_name)) +
  geom_boxplot() +
  labs(title = "Mass Distribution by Fruit Type",
       x = "Fruit Type", y = "Mass (g)") +
  theme_minimal() +
  theme(legend.position = "none")

ggplot(fruit_data, aes(x = fruit_name, y = color_score, fill = fruit_name)) +
  geom_boxplot() +
  labs(title = "Color Score Distribution by Fruit Type",
       x = "Fruit Type", y = "Color Score") +
  theme_minimal() +
  theme(legend.position = "none")

# pairs plot
pairs(fruit_data[, 4:7], col = fruit_data$fruit_label, 
      main = "Scatterplot Matrix of Fruit Features")

```

## Prepare data for KNN

```{r}
# Select features for the model
features <- fruit_data[, c("mass", "width", "height", "color_score")]

# Normalize the features (important for KNN)
preproc <- preProcess(features, method = c("center", "scale"))
features_normalized <- predict(preproc, features)

# Split the data into training and testing sets (70% train, 30% test)
set.seed(123)  # For reproducibility
trainIndex <- createDataPartition(fruit_data$fruit_label, p = 0.7, list = FALSE)
train_features <- features_normalized[trainIndex, ]
test_features <- features_normalized[-trainIndex, ]
train_labels <- fruit_data$fruit_label[trainIndex]
test_labels <- fruit_data$fruit_label[-trainIndex]

# Function to evaluate KNN with different k values
evaluate_knn <- function(k_values) {
  accuracy_results <- numeric(length(k_values))
  
  for (i in seq_along(k_values)) {
    k <- k_values[i]
    knn_pred <- knn(train = train_features, test = test_features, 
                    cl = train_labels, k = k)
    cm <- confusionMatrix(knn_pred, test_labels)
    accuracy_results[i] <- cm$overall["Accuracy"]
  }
  
  results_df <- data.frame(k = k_values, accuracy = accuracy_results)
  return(results_df)
}

# Test different k values
k_values <- c(1, 3, 5, 7, 9, 11, 13, 15)
accuracy_df <- evaluate_knn(k_values)

# Plot accuracy vs k
ggplot(accuracy_df, aes(x = k, y = accuracy)) +
  geom_line(color = "blue", size = 1) +
  geom_point(size = 3) +
  labs(title = "KNN Accuracy vs. K Value",
       x = "K Value", y = "Accuracy") +
  theme_minimal() +
  scale_x_continuous(breaks = k_values)

# Find the best k value
best_k <- accuracy_df$k[which.max(accuracy_df$accuracy)]
cat("Best k value:", best_k, "with accuracy:", max(accuracy_df$accuracy), "\n")

# Final model with the best k
final_knn_pred <- knn(train = train_features, test = test_features, 
                      cl = train_labels, k = best_k)

# Confusion matrix
conf_matrix <- confusionMatrix(final_knn_pred, test_labels)
print(conf_matrix)
```

Visualizing the decision boundaries

```{r}
# Create a grid for prediction
grid_resolution <- 100
x_range <- seq(min(features_normalized$width) - 0.5, 
               max(features_normalized$width) + 0.5, 
               length.out = grid_resolution)
y_range <- seq(min(features_normalized$height) - 0.5, 
               max(features_normalized$height) + 0.5, 
               length.out = grid_resolution)
grid <- expand.grid(width = x_range, height = y_range, 
                    mass = 0, color_score = 0)  # Set other features to their mean (0 after normalization)

# Predict classes for grid points
grid_pred <- knn(train = train_features, 
                 test = grid, 
                 cl = train_labels, 
                 k = best_k)

# Visualize decision boundaries
prediction_df <- cbind(grid, predicted = as.numeric(grid_pred))

ggplot() +
  geom_tile(data = prediction_df, 
            aes(x = width, y = height, fill = as.factor(predicted)),
            alpha = 0.3) +
  geom_point(data = cbind(features_normalized, label = fruit_data$fruit_label),
             aes(x = width, y = height, color = label),
             size = 3) +
  labs(title = paste("KNN Decision Boundaries (k =", best_k, ")"),
       x = "Normalized Width", y = "Normalized Height",
       fill = "Predicted Class", color = "Actual Class") +
  theme_minimal() +
  scale_fill_brewer(palette = "Set1") +
  scale_color_brewer(palette = "Set1")
```

Testing "mystery fruit"

```{r}
new_fruit <- data.frame(
  mass = 160,
  width = 7.2,
  height = 7.5,
  color_score = 0.76
)

# Normalize the new data using the same preprocessing
new_fruit_norm <- predict(preproc, new_fruit)

# Predict using our KNN model
prediction <- knn(train = train_features, 
                  test = new_fruit_norm, 
                  cl = train_labels, 
                  k = best_k,
                  prob = TRUE)

# Get the predicted class and confidence
predicted_class <- levels(fruit_data$fruit_name)[as.numeric(prediction)]
confidence <- attr(prediction, "prob")

cat("Mystery fruit prediction:\n")
cat("Predicted fruit type:", predicted_class, "\n")
cat("Confidence:", confidence, "\n")

# Get the actual k nearest neighbors for visualization
distances <- apply(train_features, 1, function(x) sqrt(sum((x - as.numeric(new_fruit_norm))^2)))
neighbors_idx <- order(distances)[1:best_k]

cat("\nThe", best_k, "nearest neighbors are:\n")
print(fruit_data[trainIndex[neighbors_idx], c("fruit_name", "fruit_subtype", "mass", "width", "height", "color_score")])

```

```{r}
# Visualize the mystery fruit with its neighbors
mystery_fruit_plot <- ggplot() +
  geom_point(data = fruit_data, 
             aes(x = width, y = height, color = fruit_name),
             alpha = 0.5) +
  geom_point(data = fruit_data[trainIndex[neighbors_idx], ],
             aes(x = width, y = height),
             color = "black", size = 4, shape = 21) +
  geom_point(data = new_fruit,
             aes(x = width, y = height),
             color = "red", size = 5, shape = 8) +
  labs(title = "Mystery Fruit and its K Nearest Neighbors",
       subtitle = paste("Mystery fruit (red X) and its", best_k, "nearest neighbors (black circles)"),
       x = "Width (cm)", y = "Height (cm)") +
  theme_minimal()

print(mystery_fruit_plot)
```

## Feature Importance Analysis

```{r}
# Let's see which features are most important for classification
feature_importance <- data.frame(feature = names(features))
feature_importance$importance <- 0

# A simple feature importance measure based on classification accuracy
# when using only that single feature
for (i in seq_along(names(features))) {
  feature_name <- names(features)[i]
  single_feature <- features_normalized[, feature_name, drop = FALSE]
  single_train <- single_feature[trainIndex, , drop = FALSE]
  single_test <- single_feature[-trainIndex, , drop = FALSE]
  
  knn_pred <- knn(train = single_train, test = single_test, 
                  cl = train_labels, k = best_k)
  
  feature_importance$importance[i] <- sum(knn_pred == test_labels) / length(test_labels)
}

# Sort by importance
feature_importance <- feature_importance[order(-feature_importance$importance), ]

# Plot feature importance
ggplot(feature_importance, aes(x = reorder(feature, importance), y = importance)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  labs(title = "Feature Importance for Fruit Classification",
       x = "Feature", y = "Classification Accuracy") +
  coord_flip() +
  theme_minimal()
```

## Conclusion

```{r}
cat("\n--- Summary of KNN Analysis for Fruit Classification ---\n")
cat("Total number of samples:", nrow(fruit_data), "\n")
cat("Number of fruit types:", length(unique(fruit_data$fruit_name)), "\n")
cat("Features used:", paste(names(features), collapse = ", "), "\n")
cat("Best k value:", best_k, "\n")
cat("Model accuracy:", max(accuracy_df$accuracy), "\n")
cat("Most important feature:", as.character(feature_importance$feature[1]), "\n")
cat("------------------------------------------------\n")
```
