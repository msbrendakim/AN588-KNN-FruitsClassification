<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>K-Nearest Neighbors (KNN)</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="KNN_files/libs/clipboard/clipboard.min.js"></script>
<script src="KNN_files/libs/quarto-html/quarto.js"></script>
<script src="KNN_files/libs/quarto-html/popper.min.js"></script>
<script src="KNN_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="KNN_files/libs/quarto-html/anchor.min.js"></script>
<link href="KNN_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="KNN_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="KNN_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="KNN_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="KNN_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction-to-knn" id="toc-introduction-to-knn" class="nav-link active" data-scroll-target="#introduction-to-knn">üîé Introduction to KNN</a></li>
  <li><a href="#how-knn-works-core-steps-of-knn" id="toc-how-knn-works-core-steps-of-knn" class="nav-link" data-scroll-target="#how-knn-works-core-steps-of-knn">üìå How KNN Works? Core Steps of KNN</a></li>
  <li><a href="#euclidean-distance" id="toc-euclidean-distance" class="nav-link" data-scroll-target="#euclidean-distance">üìö Euclidean Distance</a></li>
  <li><a href="#choosing-k-knn-cross-validation" id="toc-choosing-k-knn-cross-validation" class="nav-link" data-scroll-target="#choosing-k-knn-cross-validation">ü§ñ Choosing K, kNN Cross Validation</a></li>
  <li><a href="#real-dataset-fruit-sorting-with-knn" id="toc-real-dataset-fruit-sorting-with-knn" class="nav-link" data-scroll-target="#real-dataset-fruit-sorting-with-knn">ü•ù Real Dataset: Fruit Sorting with KNN</a></li>
  <li><a href="#knn-vs.-k-means" id="toc-knn-vs.-k-means" class="nav-link" data-scroll-target="#knn-vs.-k-means">üÜö KNN vs.&nbsp;K-Means</a></li>
  <li><a href="#strengths-and-limitations" id="toc-strengths-and-limitations" class="nav-link" data-scroll-target="#strengths-and-limitations">‚ùó Strengths and Limitations</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">‚úÖ Conclusion</a>
  <ul class="collapse">
  <li><a href="#key-takeaways" id="toc-key-takeaways" class="nav-link" data-scroll-target="#key-takeaways">Key Takeaways:</a></li>
  <li><a href="#when-to-use-knn" id="toc-when-to-use-knn" class="nav-link" data-scroll-target="#when-to-use-knn">When to Use KNN:</a></li>
  <li><a href="#future-directions" id="toc-future-directions" class="nav-link" data-scroll-target="#future-directions">Future Directions:</a></li>
  </ul></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
</nav>
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">K-Nearest Neighbors (KNN)</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>In this module, we will explore how the <strong>K-Nearest Neighbors (KNN)</strong> algorithm works and apply it to classify fruits using physical characteristics like <strong>mass</strong>, <strong>width</strong>, <strong>height</strong>, and <strong>color score</strong>.</p>
<section id="introduction-to-knn" class="level2">
<h2 class="anchored" data-anchor-id="introduction-to-knn">üîé Introduction to KNN</h2>
<p>KNN is an algorithm that can be used to make sorting more efficient. It works by taking in data points on a subject and predicting the classification of a new subject based on the data that‚Äôs already given. For a certain data point, it looks at the <strong>‚ÄúK‚Äù nearest points</strong> in the training data (K is a defined number by the user) and the point in question is assigned to the <strong>most common class or average value of the neighbors in the training set.</strong></p>
<p>Instead of creating a complicated model, KNN stores all the data and waits until a prediction needs to be made. When a new, unknown data point is introduced, KNN calculates the <strong>distance between this point and every other point</strong> in the dataset. It then identifies the ‚ÄúK‚Äù closest points (K is a number chosen by the user) and uses those neighbors to predict the new point‚Äôs category or value.</p>
<p>The basic idea behind KNN is that <strong>similar things are found near each other.</strong> For a classification task, once the K nearest neighbors are found, the new point is assigned to the class that appears most frequently among them.</p>
<p>If it‚Äôs a <strong>regression</strong> task, meaning you want to predict a number instead of a category, the new point‚Äôs value is usually the average of the neighboring values. KNN is called a <strong>‚Äúlazy learner‚Äù</strong> because it doesn‚Äôt do any actual learning until it‚Äôs time to make a prediction. It relies entirely on the structure of the data it‚Äôs given.</p>
<p>This can be applied to different types of sorting, like fruit sorting. For example, if you input data on the characteristics of a certain set of fruits (height, weight, mass etc.), based on this data point you can use KNN to determine the distance from this point and a new ‚Äúmystery fruit‚Äù. So, based on known data points on the characteristics of apples, you can determine if your new ‚Äúmystery fruit‚Äù is an apple or an orange based on how it fits into your identified data points.</p>
<p>So, if you input measurements for apples, oranges, and bananas, and then measure a new ‚Äúmystery fruit,‚Äù KNN can find the fruits most similar to it based on those characteristics. If, say, four of the five nearest fruits are apples, the mystery fruit would likely be classified as an apple too. In this way, KNN makes it easy to sort new items based on known examples without having to build a complex model ahead of time.</p>
<p>KNN is also called a lazy learner because it simply stores data can just make decisions when needed. That can make it easy to set up for predictions but not very efficient at making predictions for larger data sets.</p>
<p>It‚Äôs especially useful for problems where you expect that similar inputs should have similar outputs (like fruits!), and it‚Äôs flexible enough to be used in a variety of real-world situations. It‚Äôs often used by streaming services, like Amazon, Netflix, or Hulu, to gage what kind of shows users prefer. In healthcare, KNN can help predict medical diagnoses by comparing a patient‚Äôs symptoms to past cases by indentifying similar symptoms. It has also been used in various instances of image recognition, like identifying handwritten numbers or objects in photos, and in banking, where it can spot unusual transactions that might be fraud.</p>
<hr>
</section>
<section id="how-knn-works-core-steps-of-knn" class="level2">
<h2 class="anchored" data-anchor-id="how-knn-works-core-steps-of-knn">üìå How KNN Works? Core Steps of KNN</h2>
<ol type="1">
<li><p><strong>Choose</strong> the number of neighbors <code>K</code> (usually an odd number like 3, 5, or 7).</p>
<p>The first thing you do is pick how many neighbors you want the algorithm to consider when it makes a prediction. K is just a positive whole number, like 3, 5, or 7. Most people go with an odd number to avoid ties when you‚Äôre voting on the most common class, so you don‚Äôt end up with a tie as to what your result is. A popular trick is to use the square root of the total number of data points in your dataset to pick K. A smaller K means the model is more sensitive to little changes and might get thrown off by outliers. A bigger K value smooths things out a bit more and makes the model more stable, but if K is too large, it can start blending things together that maybe shouldn‚Äôt be. So picking the right K really depends on the data.</p></li>
<li><p><strong>Calculate</strong> distance (e.g., Euclidean) from the new point to all existing data points.</p>
<p>Once you‚Äôve chosen K, the next step is to figure out how close your new, unknown data point is to all the other points in your training data. This is where distance metrics come in. The most common one is Euclidean distance, which is just the straight-line distance between two points. You could also use Manhattan distance, which moves along a grid, or Minkowski distance, which is a more general formula. Essentially, you‚Äôre measuring how similar or different the new point is from everything else „ÖÅbased on the features you‚Äôre tracking, like weight, height, or whatever.</p></li>
<li><p><strong>Identify</strong> the <code>K</code> closest neighbors.</p>
<p>Once all the distances are calculated, you sort the training data by how close each point is to your new one. Then you just take the K points that are closest. These are the neighbors that are going to help decide what label or value the new point gets. The idea is that points that are close together in this space are likely to be similar.</p></li>
<li><p><strong>Predict</strong> by majority vote (for classification) or average value (for regression).</p>
<p>Now that you have your K neighbors, you use them to actually make a prediction. If you‚Äôre doing classification, you look at the labels of the K neighbors and go with the one that shows up the most. Like if you‚Äôre trying to classify a mystery fruit and 3 of the 5 nearest neighbors are apples, then you‚Äôll probably call it an apple. If you‚Äôre doing regression, which is when you‚Äôre trying to predict a number, you just take the average of the neighbors‚Äô values and use that as your prediction for the new point.</p></li>
</ol>
<blockquote class="blockquote">
<p><strong>Example:</strong><br>
If you‚Äôre given data on the size and color of known apples, oranges, lemons, etc., and then introduced to a new ‚Äúmystery fruit‚Äù, KNN can tell you what kind of fruit it most likely is ‚Äî based on how similar it is to others.</p>
</blockquote>
<hr>
</section>
<section id="euclidean-distance" class="level2">
<h2 class="anchored" data-anchor-id="euclidean-distance">üìö Euclidean Distance</h2>
<p>The distance between points is a key part of KNN. This is essential because it helps us know who is ‚Äúcloset‚Äù so you can pick right neighbor. The most commonly used metric is Euclidean distance for continuous variables, but you could also use others like Manhattan distance.</p>
<p>The Euclidean distance is most commonly used because it is:</p>
<ol type="1">
<li>Easy to compute</li>
<li>Makes geometric sense for continuous variables (eg. height, weight, age, etc)</li>
<li>Matches our intuitive notion of ‚Äúnearness‚Äù in a regular space</li>
</ol>
<p>The Euclidean Distance is the ‚Äústraight-line‚Äù distance between two points in Euclidean space. In KNN and many ML algorithms, we treat data points as if they live in <strong>n-dimensional Euclidean space</strong>, where each <strong>feature</strong> (or variable) is a <strong>dimension</strong>. So if a dataset has 4 features (like height, weight, age, income), it lives in <strong>4D Euclidean space</strong>. We can then calculate Euclidean distances between points to compare them.git config pull.rebase false</p>
<p>For two points A = (x‚ÇÅ, y‚ÇÅ) and B = (x‚ÇÇ, y‚ÇÇ), <span class="math inline">\(d = \sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2}\)</span>, the <strong>Euclidean distance</strong> is: <span class="math inline">\(d(A,B) = \sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2 + ... + (n_1 - n_2)^2}\)</span></p>
<p>In multi-feature datasets, each feature is a dimension in the space.</p>
<section id="example-codes" class="level4">
<h4 class="anchored" data-anchor-id="example-codes">Example Codes</h4>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load necessary package</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co"># install.packages("class")  # run this once if not already installed</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(class)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Training data (2D points)</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>X_train <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>,</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>                    <span class="dv">2</span>, <span class="dv">3</span>,</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>                    <span class="dv">3</span>, <span class="dv">1</span>,</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>                    <span class="dv">6</span>, <span class="dv">5</span>,</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>                    <span class="dv">7</span>, <span class="dv">7</span>), </span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>                  <span class="at">ncol =</span> <span class="dv">2</span>, <span class="at">byrow =</span> <span class="cn">TRUE</span>)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Labels for training data</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>y_train <span class="ot">&lt;-</span> <span class="fu">factor</span>(<span class="fu">c</span>(<span class="st">"A"</span>, <span class="st">"A"</span>, <span class="st">"A"</span>, <span class="st">"B"</span>, <span class="st">"B"</span>))</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="co"># New data point to classify</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>X_test <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">3</span>, <span class="dv">2</span>), <span class="at">ncol =</span> <span class="dv">2</span>)</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Run KNN (k = 3)</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>predicted_label <span class="ot">&lt;-</span> <span class="fu">knn</span>(<span class="at">train =</span> X_train, <span class="at">test =</span> X_test, <span class="at">cl =</span> y_train, <span class="at">k =</span> <span class="dv">3</span>)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Output the prediction</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">paste</span>(<span class="st">"Predicted label:"</span>, predicted_label))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Predicted label: A"</code></pre>
</div>
</div>
<hr>
</section>
</section>
<section id="choosing-k-knn-cross-validation" class="level2">
<h2 class="anchored" data-anchor-id="choosing-k-knn-cross-validation">ü§ñ Choosing K, kNN Cross Validation</h2>
<p>In KNN, choosing the right value of K (the number of neighbors) is extremely important because it controls the model‚Äôs complexity:</p>
<ul>
<li><p><strong>Small K (Risk of overfitting):</strong></p>
<ol type="1">
<li>The model becomes <strong>very sensitive to noise</strong> and small fluctuations in the data.</li>
<li>It may fit <strong>outliers</strong> and <strong>mistakes</strong> in the training set.</li>
<li>This can cause <strong>overfitting</strong>, where the model memorizes the training data too closely but performs poorly on new, unseen data.</li>
</ol></li>
<li><p><strong>Large K (Risk of underfitting.):</strong></p>
<ol type="1">
<li><p>The model looks at <strong>more neighbors</strong> and becomes <strong>more stable</strong> and <strong>smoother</strong>.</p></li>
<li><p>It may <strong>ignore important local patterns</strong> by averaging too much.</p></li>
<li><p>This can cause <strong>underfitting</strong>, where the model is too simple and cannot capture the complexity of the data.</p></li>
</ol></li>
</ul>
<p>Thus, <strong>choosing the best K is a balance</strong> between overfitting and underfitting. Cross-validation is often used to find the optimal K.&nbsp;</p>
<section id="why-use-cross-validation-to-choose-k" class="level4">
<h4 class="anchored" data-anchor-id="why-use-cross-validation-to-choose-k"><strong>Why use Cross-Validation to choose K?</strong></h4>
<ul>
<li><p>You cannot just guess the best K value.</p></li>
<li><p><strong>Cross-validation</strong> helps systematically test different K values on parts of the data not used for training, simulating how the model would perform on truly unseen data.</p></li>
<li><p>You typically try a <strong>range of K values</strong> (e.g., from 1 to 20) and select the one that gives the <strong>highest validation accuracy</strong>.</p></li>
</ul>
<p>Therefore, try multiple K values and use cross-validation, this is the most common and reliable method.</p>
</section>
<section id="example-codes-1" class="level4">
<h4 class="anchored" data-anchor-id="example-codes-1">Example codes</h4>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Install packages if needed</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="co"># install.packages(‚Äúcaret‚Äù)</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Example in R using iris dataset</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(class)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Loading required package: ggplot2</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Loading required package: lattice</code></pre>
</div>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the data</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>train_index <span class="ot">&lt;-</span> <span class="fu">createDataPartition</span>(iris<span class="sc">$</span>Species, <span class="at">p =</span> <span class="fl">0.7</span>, <span class="at">list =</span> <span class="cn">FALSE</span>)</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>train_data <span class="ot">&lt;-</span> iris[train_index, ]</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>test_data <span class="ot">&lt;-</span> iris[<span class="sc">-</span>train_index, ]</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Scale features</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>train_scaled <span class="ot">&lt;-</span> <span class="fu">scale</span>(train_data[, <span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>])</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>test_scaled <span class="ot">&lt;-</span> <span class="fu">scale</span>(test_data[, <span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>], <span class="at">center =</span> <span class="fu">attr</span>(train_scaled, <span class="st">"scaled:center"</span>), </span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>                                         <span class="at">scale =</span> <span class="fu">attr</span>(train_scaled, <span class="st">"scaled:scale"</span>))</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Try different K values and record accuracy</span></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>accuracies <span class="ot">&lt;-</span> <span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">20</span>, <span class="cf">function</span>(k) {</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>  pred <span class="ot">&lt;-</span> <span class="fu">knn</span>(<span class="at">train =</span> train_scaled, <span class="at">test =</span> test_scaled, <span class="at">cl =</span> train_data<span class="sc">$</span>Species, <span class="at">k =</span> k)</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mean</span>(pred <span class="sc">==</span> test_data<span class="sc">$</span>Species)</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot accuracy vs K</span></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">20</span>, accuracies, <span class="at">type =</span> <span class="st">"b"</span>, <span class="at">col =</span> <span class="st">"blue"</span>, <span class="at">pch =</span> <span class="dv">19</span>,</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"K"</span>, <span class="at">ylab =</span> <span class="st">"Accuracy"</span>, <span class="at">main =</span> <span class="st">"KNN Accuracy for Different K"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="KNN_files/figure-html/unnamed-chunk-2-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>This plot illustrates how the accuracy of the K-Nearest Neighbors (KNN) algorithm varies with different values of K‚Äîthe number of nearest neighbors considered during classification. As K increases, the model becomes more generalized, averaging the behavior of more neighboring points. Conversely, smaller K values make the model more sensitive to local patterns and noise in the data.</p>
<hr>
</section>
</section>
<section id="real-dataset-fruit-sorting-with-knn" class="level2">
<h2 class="anchored" data-anchor-id="real-dataset-fruit-sorting-with-knn">ü•ù Real Dataset: Fruit Sorting with KNN</h2>
<p>Now we‚Äôre going to apply the KNN algorithm to a real dataset. We‚Äôve learned the theoretical aspects so far, but now let‚Äôs see how it actually works in practice! We‚Äôll walk through the code step-by-step, interpret outputs like scatterplots, accuracy plots, and confusion matrices, and answer key questions like: What‚Äôs a confusion matrix? Why normalize data? Why split it? And why compare models?</p>
<section id="dataset-loading-and-setup" class="level4">
<h4 class="anchored" data-anchor-id="dataset-loading-and-setup">Dataset Loading and Setup</h4>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(readr)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>
Attaching package: 'dplyr'</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>The following objects are masked from 'package:stats':

    filter, lag</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>The following objects are masked from 'package:base':

    intersect, setdiff, setequal, union</code></pre>
</div>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(class)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret)</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>fruit_data <span class="ot">&lt;-</span> <span class="fu">read.delim</span>(<span class="st">"fruit_data_with_colors.txt"</span>, <span class="at">sep=</span><span class="st">"</span><span class="sc">\t</span><span class="st">"</span>, <span class="at">header=</span><span class="cn">TRUE</span>)</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>fruit_data<span class="sc">$</span>fruit_label <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(fruit_data<span class="sc">$</span>fruit_label)</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>fruit_data<span class="sc">$</span>fruit_name <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(fruit_data<span class="sc">$</span>fruit_name)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We start by loading our fruit dataset, stored in fruit_data_with_colors.txt, a tab-separated file with measurements like mass and color score. Here, we convert fruit_label and fruit_name to factors, which tells R these are categories (e.g., apple, orange), not numbers. This is crucial for KNN, a classification algorithm that assigns categories based on nearby points.</p>
</section>
<section id="data-exploration" class="level4">
<h4 class="anchored" data-anchor-id="data-exploration">Data Exploration</h4>
<div class="cell">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fruit_data)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> fruit_label    fruit_name fruit_subtype           mass           width      
 1:19        apple   :19   Length:59          Min.   : 76.0   Min.   :5.800  
 2: 5        lemon   :16   Class :character   1st Qu.:140.0   1st Qu.:6.600  
 3:19        mandarin: 5   Mode  :character   Median :158.0   Median :7.200  
 4:16        orange  :19                      Mean   :163.1   Mean   :7.105  
                                              3rd Qu.:177.0   3rd Qu.:7.500  
                                              Max.   :362.0   Max.   :9.600  
     height        color_score    
 Min.   : 4.000   Min.   :0.5500  
 1st Qu.: 7.200   1st Qu.:0.7200  
 Median : 7.600   Median :0.7500  
 Mean   : 7.693   Mean   :0.7629  
 3rd Qu.: 8.200   3rd Qu.:0.8100  
 Max.   :10.500   Max.   :0.9300  </code></pre>
</div>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(fruit_data<span class="sc">$</span>fruit_name)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
   apple    lemon mandarin   orange 
      19       16        5       19 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(fruit_data, <span class="fu">aes</span>(<span class="at">x =</span> width, <span class="at">y =</span> height, <span class="at">color =</span> fruit_name)) <span class="sc">+</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">3</span>, <span class="at">alpha =</span> <span class="fl">0.7</span>) <span class="sc">+</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Fruit Types by Size"</span>) <span class="sc">+</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="KNN_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Before we model, we need to understand our data. The summary(fruit_data) output shows stats for our features. For example, mass ranges from 76 to 362 grams, and color score is between 0.55 and 0.93. This tells us our features have different scales, which we‚Äôll need to fix later for KNN.</p>
<p>The table(fruit_data$fruit_name) output shows we have 19 apples, 16 lemons, 5 mandarins, and 19 oranges. Notice mandarins are underrepresented with only 5 samples, which might make them harder to classify accurately compared to apples or oranges.</p>
<p>The scatterplot plots width versus height, with each point colored differently. When you look at this plot, you might see mandarins clustering in one area (shorter and narrower) and lemons in another (taller and narrower). If the fruit types form distinct clusters, that‚Äôs a great sign for KNN, because it relies on nearby points being similar. But if points overlap a lot, like apples blending with oranges, KNN might struggle to separate them. This plot gives us a first look at whether our features (width and height) are good for distinguishing fruits, and it hints that mass and color score might add even more separation.</p>
</section>
<section id="feature-normalization-and-splitting" class="level4">
<h4 class="anchored" data-anchor-id="feature-normalization-and-splitting">Feature Normalization and Splitting</h4>
<div class="cell">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>features <span class="ot">&lt;-</span> fruit_data[, <span class="fu">c</span>(<span class="st">"mass"</span>, <span class="st">"width"</span>, <span class="st">"height"</span>, <span class="st">"color_score"</span>)]</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>preproc <span class="ot">&lt;-</span> <span class="fu">preProcess</span>(features, <span class="at">method =</span> <span class="fu">c</span>(<span class="st">"center"</span>, <span class="st">"scale"</span>))</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>features_scaled <span class="ot">&lt;-</span> <span class="fu">predict</span>(preproc, features)</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>train_idx <span class="ot">&lt;-</span> <span class="fu">createDataPartition</span>(fruit_data<span class="sc">$</span>fruit_label, <span class="at">p =</span> <span class="fl">0.7</span>, <span class="at">list =</span> <span class="cn">FALSE</span>)</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>train_x <span class="ot">&lt;-</span> features_scaled[train_idx, ]</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>test_x <span class="ot">&lt;-</span> features_scaled[<span class="sc">-</span>train_idx, ]</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>train_y <span class="ot">&lt;-</span> fruit_data<span class="sc">$</span>fruit_label[train_idx]</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>test_y <span class="ot">&lt;-</span> fruit_data<span class="sc">$</span>fruit_label[<span class="sc">-</span>train_idx]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Here, we‚Äôre prepping our data for KNN. We select our features‚Äîmass, width, height, and color score‚Äîinto features. Since KNN calculates distances, we need to normalize these features because they‚Äôre on different scales (e.g., mass in grams vs.&nbsp;color score from 0 to 1). The preProcess function with method = c(‚Äúcenter‚Äù, ‚Äúscale‚Äù) standardizes each feature to have a mean of 0 and a standard deviation of 1, and predict(preproc, features) applies this to get features_scaled.</p>
<p>Then, we split the data into 70% training and 30% testing sets using createDataPartition. This tests how well our model generalizes to unseen data, mimicking real-world scenarios where new fruits arrive. The set.seed(123) ensures our split is reproducible, and the split keeps the fruit classes balanced. train_x and train_y are our training features and labels, while test_x and test_y are for testing. This lets us train our model and test it on unseen data to check its generalization.</p>
</section>
<section id="model-evaluation-choosing-best-k" class="level4">
<h4 class="anchored" data-anchor-id="model-evaluation-choosing-best-k">Model Evaluation: Choosing Best K</h4>
<div class="cell">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>k_values <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">1</span>, <span class="dv">15</span>, <span class="dv">2</span>)</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>accuracy_list <span class="ot">&lt;-</span> <span class="fu">sapply</span>(k_values, <span class="cf">function</span>(k) {</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>  pred <span class="ot">&lt;-</span> <span class="fu">knn</span>(train_x, test_x, train_y, k)</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mean</span>(pred <span class="sc">==</span> test_y)</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(k_values, accuracy_list, <span class="at">type =</span> <span class="st">"b"</span>, <span class="at">col =</span> <span class="st">"blue"</span>, <span class="at">pch =</span> <span class="dv">19</span>,</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"K"</span>, <span class="at">ylab =</span> <span class="st">"Accuracy"</span>, <span class="at">main =</span> <span class="st">"KNN Accuracy by K"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="KNN_files/figure-html/unnamed-chunk-6-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>max_accuracy <span class="ot">&lt;-</span> <span class="fu">max</span>(accuracy_list)</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>threshold <span class="ot">&lt;-</span> <span class="fl">0.02</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>good_k <span class="ot">&lt;-</span> k_values[accuracy_list <span class="sc">&gt;=</span> (max_accuracy <span class="sc">-</span> threshold)]</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>best_k <span class="ot">&lt;-</span> <span class="fu">max</span>(good_k)</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Best K (within 0.02 of max accuracy):"</span>, best_k, <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Best K (within 0.02 of max accuracy): 7 </code></pre>
</div>
</div>
<p>We test K values (1, 3, 5, ‚Ä¶, 15) to find the best number of neighbors. For each K, knn predicts test set labels, and we compute accuracy. The plot shows accuracy versus K. We pick the absolute highest accuracy (K=7, 1.0), with an accuracy of ~0.98.</p>
<p>The plot also shows accuracy peaking at K=1 (1.0), but K=1 risks overfitting. K=7, with an accuracy of ~0.98, is a better choice for a fruit sorting system, as it uses 7 neighbors, reducing sensitivity to noise while maintaining high accuracy. This means we might misclassify 1 out of ~18 test fruits, but the model will generalize better to new fruits.</p>
</section>
<section id="final-model-and-confusion-matrix" class="level4">
<h4 class="anchored" data-anchor-id="final-model-and-confusion-matrix">Final Model and Confusion Matrix</h4>
<div class="cell">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>final_pred <span class="ot">&lt;-</span> <span class="fu">knn</span>(train_x, test_x, train_y, best_k)</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>cm <span class="ot">&lt;-</span> <span class="fu">confusionMatrix</span>(final_pred, test_y)</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Precision, Recall, F1-score by class</span></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>metrics <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">Class =</span> <span class="fu">levels</span>(test_y),</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">Precision =</span> cm<span class="sc">$</span>byClass[, <span class="st">"Pos Pred Value"</span>],</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>  <span class="at">Recall =</span> cm<span class="sc">$</span>byClass[, <span class="st">"Sensitivity"</span>],</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>  <span class="at">F1 =</span> <span class="dv">2</span> <span class="sc">*</span> ((cm<span class="sc">$</span>byClass[, <span class="st">"Pos Pred Value"</span>] <span class="sc">*</span> cm<span class="sc">$</span>byClass[, <span class="st">"Sensitivity"</span>]) <span class="sc">/</span></span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>            (cm<span class="sc">$</span>byClass[, <span class="st">"Pos Pred Value"</span>] <span class="sc">+</span> cm<span class="sc">$</span>byClass[, <span class="st">"Sensitivity"</span>]))</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>metrics</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>         Class Precision Recall F1
Class: 1     1         1      1  1
Class: 2     2         1      1  1
Class: 3     3         1      1  1
Class: 4     4         1      1  1</code></pre>
</div>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># label map</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>label_map <span class="ot">&lt;-</span> <span class="fu">levels</span>(fruit_data<span class="sc">$</span>fruit_name)</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(label_map) <span class="ot">&lt;-</span> <span class="fu">levels</span>(fruit_data<span class="sc">$</span>fruit_label)</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a><span class="co"># confusion matrix transfer to heatmap</span></span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>cm_df <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(cm<span class="sc">$</span>table)</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>cm_df<span class="sc">$</span>Reference <span class="ot">&lt;-</span> <span class="fu">factor</span>(cm_df<span class="sc">$</span>Reference, <span class="at">levels =</span> <span class="fu">names</span>(label_map), <span class="at">labels =</span> label_map)</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>cm_df<span class="sc">$</span>Prediction <span class="ot">&lt;-</span> <span class="fu">factor</span>(cm_df<span class="sc">$</span>Prediction, <span class="at">levels =</span> <span class="fu">names</span>(label_map), <span class="at">labels =</span> label_map)</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a><span class="co"># heatmap visualization</span></span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(cm_df, <span class="fu">aes</span>(<span class="at">x =</span> Reference, <span class="at">y =</span> Prediction, <span class="at">fill =</span> Freq)) <span class="sc">+</span></span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_tile</span>(<span class="at">color =</span> <span class="st">"white"</span>) <span class="sc">+</span></span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_text</span>(<span class="fu">aes</span>(<span class="at">label =</span> Freq), <span class="at">color =</span> <span class="st">"black"</span>, <span class="at">size =</span> <span class="dv">6</span>) <span class="sc">+</span></span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_fill_gradient</span>(<span class="at">low =</span> <span class="st">"white"</span>, <span class="at">high =</span> <span class="st">"steelblue"</span>) <span class="sc">+</span></span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"KNN Confusion Matrix (Fruit Names)"</span>, <span class="at">x =</span> <span class="st">"True Label"</span>, <span class="at">y =</span> <span class="st">"Predicted Label"</span>) <span class="sc">+</span></span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="KNN_files/figure-html/unnamed-chunk-7-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>We use best_k (K=7) to run our final KNN model with knn, making predictions on the test set (final_pred). The confusionMatrix compares these predictions to test_y, giving us a table of correct and incorrect classifications. We also compute precision, recall, and F1-score per class. Precision is how often our ‚Äúapple‚Äù predictions are correct; recall is how many actual apples we identified; F1 balances both. Finally, we create a heatmap of the confusion matrix, mapping numeric labels (1, 2, 3, 4) to fruit names (apple, lemon, mandarin, orange) for clarity.</p>
<p>The metrics table shows perfect scores (1.0 for precision, recall, and F1) for all classes. This looks amazing, but it‚Äôs suspicious! Perfect scores often mean the test set is too small (only ~18 fruits with 30% of 59) or the data is too clean, making the work flawlessly. In a real fruit sorting system, new fruits might have more variability (e.g., bruised apples), so this perfection might not hold. The heatmap confirms this: the diagonal (correct predictions, like apple predicted as apple) has all the counts, and off-diagonal cells (mistakes) are zero. This suggests our model nailed the test set, but we should be cautious‚Äîit might not generalize to new fruits. For example, mandarins, with only 5 samples, might be overfitted, as there‚Äôs little data to learn from.</p>
</section>
<section id="model-comparison-knn-vs-logistic-regression" class="level4">
<h4 class="anchored" data-anchor-id="model-comparison-knn-vs-logistic-regression">üîÄ Model Comparison: KNN vs Logistic Regression</h4>
<p>A model comparison table helps us evaluate which algorithm is best suited for our task.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(nnet)  <span class="co"># for multinom()</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(e1071) <span class="co"># confusionMatrix</span></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a><span class="co"># using the same features in data</span></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>train_df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(train_x)</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>train_df<span class="sc">$</span>label <span class="ot">&lt;-</span> train_y</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>test_df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(test_x)</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>test_df<span class="sc">$</span>label <span class="ot">&lt;-</span> test_y</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Multinomial logistic regression</span></span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>logit_model <span class="ot">&lt;-</span> <span class="fu">multinom</span>(label <span class="sc">~</span> ., <span class="at">data =</span> train_df)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># weights:  24 (15 variable)
initial  value 60.996952 
iter  10 value 13.831555
iter  20 value 12.482299
iter  30 value 12.435228
final  value 12.434969 
converged</code></pre>
</div>
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="co"># prediction</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>logit_pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(logit_model, <span class="at">newdata =</span> test_df)</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>cm_logit <span class="ot">&lt;-</span> <span class="fu">confusionMatrix</span>(logit_pred, test_y)</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a><span class="co"># comparison</span></span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>cm_logit_df <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(cm_logit<span class="sc">$</span>table)</span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>cm_logit_df<span class="sc">$</span>Reference <span class="ot">&lt;-</span> <span class="fu">factor</span>(cm_logit_df<span class="sc">$</span>Reference, <span class="at">levels =</span> <span class="fu">names</span>(label_map), <span class="at">labels =</span> label_map)</span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>cm_logit_df<span class="sc">$</span>Prediction <span class="ot">&lt;-</span> <span class="fu">factor</span>(cm_logit_df<span class="sc">$</span>Prediction, <span class="at">levels =</span> <span class="fu">names</span>(label_map), <span class="at">labels =</span> label_map)</span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(cm_logit_df, <span class="fu">aes</span>(<span class="at">x =</span> Reference, <span class="at">y =</span> Prediction, <span class="at">fill =</span> Freq)) <span class="sc">+</span></span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_tile</span>(<span class="at">color =</span> <span class="st">"white"</span>) <span class="sc">+</span></span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_text</span>(<span class="fu">aes</span>(<span class="at">label =</span> Freq), <span class="at">color =</span> <span class="st">"black"</span>, <span class="at">size =</span> <span class="dv">6</span>) <span class="sc">+</span></span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_fill_gradient</span>(<span class="at">low =</span> <span class="st">"white"</span>, <span class="at">high =</span> <span class="st">"tomato"</span>) <span class="sc">+</span></span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Logistic Regression Confusion Matrix (Fruit Names)"</span>, <span class="at">x =</span> <span class="st">"True Label"</span>, <span class="at">y =</span> <span class="st">"Predicted Label"</span>) <span class="sc">+</span></span>
<span id="cb26-16"><a href="#cb26-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="KNN_files/figure-html/unnamed-chunk-8-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="model-comparison-table" class="level4">
<h4 class="anchored" data-anchor-id="model-comparison-table">Model Comparison Table</h4>
<div class="cell">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>comparison_df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">Model =</span> <span class="fu">c</span>(<span class="st">"KNN"</span>, <span class="st">"Logistic Regression"</span>),</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">Accuracy =</span> <span class="fu">c</span>(cm<span class="sc">$</span>overall[<span class="st">"Accuracy"</span>], cm_logit<span class="sc">$</span>overall[<span class="st">"Accuracy"</span>]),</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">Kappa =</span> <span class="fu">c</span>(cm<span class="sc">$</span>overall[<span class="st">"Kappa"</span>], cm_logit<span class="sc">$</span>overall[<span class="st">"Kappa"</span>])</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>comparison_df</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                Model Accuracy     Kappa
1                 KNN      1.0 1.0000000
2 Logistic Regression      0.8 0.7151899</code></pre>
</div>
</div>
<p>Seeing the model comparison table, KNN, with an accuracy of 1.0, got all test fruits correct, while logistic regression, with 0.8 accuracy, got 14‚Äì15 out of 18 correct.</p>
<p>Kappa measures agreement between predicted and actual labels, adjusted for chance. A Kappa of 1.0 (KNN) means perfect agreement, while 0.7151 (logistic regression) indicates good but not perfect agreement, likely due to errors in smaller classes like mandarins, where chance agreement is lower.</p>
<p>You might be wondering, ‚Äú<em>How did KNN get a perfect score like this?</em>‚Äù</p>
<ul>
<li><p>KNN classifies fruits by finding the 7 nearest neighbors in feature space (mass, width, height, color score). Our dataset is small (59 fruits, ~18 in the test set), and the scatterplot showed distinct clusters (e.g., apples vs.&nbsp;lemons). With K=7, KNN likely found clear majorities for each test fruit, leading to perfect predictions. However, the earlier accuracy plot showed ~0.98 for K=7, so this perfect 1.0 suggests either a different random split or a very small test set where K=7 happened to align perfectly.</p></li>
<li><p>Logistic regression assumes linear boundaries between classes. If mandarins and oranges overlap in size or color score, it might misclassify them, leading to errors (e.g., 3‚Äì4 misclassifications for 0.8 accuracy). Its Kappa of 0.7151 shows it‚Äôs still decent but struggles with non-linear patterns, which KNN handles better.</p></li>
<li><p>With only ~18 test fruits, small changes in predictions have a big impact. KNN getting 18/18 correct gives 1.0, while logistic regression getting 14/18 gives 0.8‚Äîa difference of just 4 fruits but a large gap in percentage. This highlights the risk of overfitting with KNN, especially on a small dataset.</p></li>
</ul>
<hr>
</section>
<section id="classifying-a-mystery-fruit" class="level4">
<h4 class="anchored" data-anchor-id="classifying-a-mystery-fruit">ü•≠ Classifying a Mystery Fruit</h4>
<div class="cell">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>new_fruit <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">mass =</span> <span class="dv">160</span>, <span class="at">width =</span> <span class="fl">7.2</span>, <span class="at">height =</span> <span class="fl">7.5</span>, <span class="at">color_score =</span> <span class="fl">0.76</span>)</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>new_scaled <span class="ot">&lt;-</span> <span class="fu">predict</span>(preproc, new_fruit)</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>mystery_pred <span class="ot">&lt;-</span> <span class="fu">knn</span>(train_x, new_scaled, train_y, <span class="at">k =</span> best_k, <span class="at">prob =</span> <span class="cn">TRUE</span>)</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Prediction:"</span>, mystery_pred)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Prediction: 3</code></pre>
</div>
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Confidence:"</span>, <span class="fu">attr</span>(mystery_pred, <span class="st">"prob"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Confidence: 0.8571429</code></pre>
</div>
</div>
<p>Finally, we classify a mystery fruit with mass=160g, width=7.2, height=7.5, and color_score=0.76. We normalize it using the same preproc object to match our training data‚Äôs scale, then run knn with best_k=7.</p>
<p>The prediction is 3, which maps to ‚Äúmandarin‚Äù (based on label_map). The confidence is approximately 0.86, meaning the closest neighbor (since K=7) is 86% a mandarin. In a real-world scenario, like a fruit sorting machine, we‚Äôd want a higher K for reliability, as one odd fruit could throw us off. This result shows KNN‚Äôs power for quick predictions but also highlights the risk of overfitting with a small K.</p>
<hr>
</section>
</section>
<section id="knn-vs.-k-means" class="level2">
<h2 class="anchored" data-anchor-id="knn-vs.-k-means">üÜö KNN vs.&nbsp;K-Means</h2>
<table class="caption-top table">
<colgroup>
<col style="width: 18%">
<col style="width: 40%">
<col style="width: 40%">
</colgroup>
<thead>
<tr class="header">
<th>Feature</th>
<th>KNN</th>
<th>K-Means</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Supervised?</td>
<td>Yes</td>
<td>No</td>
</tr>
<tr class="even">
<td>Used For</td>
<td>Classification / Regression</td>
<td>Clustering</td>
</tr>
<tr class="odd">
<td>Distance Use</td>
<td>Classify based on neighbors</td>
<td>Assign to closest centroid</td>
</tr>
<tr class="even">
<td>Label Needed?</td>
<td>Yes</td>
<td>No</td>
</tr>
<tr class="odd">
<td>Type</td>
<td>Lazy Learner (no training phase)</td>
<td>Iterative optimization algorithm</td>
</tr>
</tbody>
</table>
<hr>
</section>
<section id="strengths-and-limitations" class="level2">
<h2 class="anchored" data-anchor-id="strengths-and-limitations">‚ùó Strengths and Limitations</h2>
<table class="caption-top table">
<colgroup>
<col style="width: 48%">
<col style="width: 51%">
</colgroup>
<thead>
<tr class="header">
<th>Advantages</th>
<th>Limitations</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><ul>
<li><p>Easy to understand and implement</p></li>
<li><p>No training phase needed</p></li>
<li><p>Flexible for multi-class problems</p></li>
</ul></td>
<td><ul>
<li><p>Expensive at prediction time</p></li>
<li><p>Sensitive to irrelevant features</p></li>
<li><p>Struggles in high-dimensional space</p></li>
</ul></td>
</tr>
</tbody>
</table>
<hr>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">‚úÖ Conclusion</h2>
<p>In this module, we explored the K-Nearest Neighbors (KNN) algorithm from its theoretical foundation to practical implementation using a fruit classification task. Through both toy examples and real datasets, we gained a deeper understanding of how similarity-based classification works.</p>
<section id="key-takeaways" class="level3">
<h3 class="anchored" data-anchor-id="key-takeaways">Key Takeaways:</h3>
<ul>
<li><strong>KNN is a non-parametric, instance-based learning method</strong> that predicts new data points by comparing their distance to known labeled data.</li>
<li>Normalization of features was critical, especially since Euclidean distance is sensitive to scale.</li>
<li>Model performance varied with the value of <strong>K</strong>, with <code>K = 1</code> showing perfect classification for this particular dataset. However, this could lead to overfitting in noisier or more complex data.</li>
<li>Visualization using <strong>confusion matrices</strong> helped us interpret model results at a glance, especially when labeled with meaningful class names (like apple, orange, etc.).</li>
<li>We compared KNN to <strong>Logistic Regression</strong>, a probabilistic model, and found that while KNN achieved perfect accuracy in this specific task, logistic regression showed slightly lower accuracy and Kappa. This demonstrates the potential of KNN for small, structured, and well-separated datasets.</li>
</ul>
</section>
<section id="when-to-use-knn" class="level3">
<h3 class="anchored" data-anchor-id="when-to-use-knn">When to Use KNN:</h3>
<p>KNN is particularly useful when: - You want a simple baseline model - You have a small to medium-sized dataset - Interpretability and flexibility are more important than speed</p>
<p>However, its limitations in terms of: - Computational cost (especially with large datasets) - Sensitivity to irrelevant features or high dimensionality should be kept in mind.</p>
</section>
<section id="future-directions" class="level3">
<h3 class="anchored" data-anchor-id="future-directions">Future Directions:</h3>
<ul>
<li><p>Try <strong>changing the distance metric</strong> (e.g., Manhattan, cosine) and evaluate impact.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 21%">
<col style="width: 78%">
</colgroup>
<thead>
<tr class="header">
<th>Distance Type</th>
<th>When to use</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Euclidean Distance</td>
<td>Continuous, real-valued features (e.g., height, weight, income)</td>
</tr>
<tr class="even">
<td>Manhattan Distance</td>
<td>Grid-like spaces (e.g., city blocks)</td>
</tr>
<tr class="odd">
<td>Cosine Similarity</td>
<td>When you care about <strong>direction</strong> more than <strong>distance</strong> (e.g., text data)</td>
</tr>
<tr class="even">
<td>Hamming Distance</td>
<td>Categorical data (e.g., bit strings)</td>
</tr>
</tbody>
</table></li>
<li><p>Apply <strong>dimensionality reduction</strong> techniques like PCA to see how performance and interpretability are affected.</p></li>
<li><p>Compare KNN with other classifiers such as Decision Trees, Random Forests, or SVMs for larger datasets or more complex feature spaces.</p></li>
</ul>
<p>In summary, KNN provides a solid foundation for understanding proximity-based machine learning and offers valuable intuition for feature-space dynamics in classification tasks.</p>
<hr>
</section>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<ul>
<li>‚ÄúK-Nearest Neighbors (KNN)‚Äù - <a href="https://scikit-learn.org/stable/modules/neighbors.html">Scikit-Learn User Guide</a></li>
<li>Rahul Agarwal‚Äôs <a href="https://www.kaggle.com/code/rahul253801/fruits-classification/notebook">Fruit Classification on Kaggle</a></li>
<li>Inspired by <a href="https://fuzzyatelin.github.io/bioanth-stats/module-25/module-25.html">Cluster Analysis Module</a></li>
<li>Lecture notes from BI_AN588 (2024)</li>
</ul>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>